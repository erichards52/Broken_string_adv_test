/* 
 * Enable DSL 2 syntax
 */
nextflow.enable.dsl = 2


////////////////////////////////////////////////////
/* --    IMPORT LOCAL MODULES/SUBWORKFLOWS     -- */
////////////////////////////////////////////////////

include { FILTER_READS            } from './modules/filter_reads/main'
include { INTERSECT_COUNTS        } from './modules/intersect_counts/main'
include { COUNT_SUM_NORM          } from './modules/count_sum_norm/main'
include { COMBINE_OUTPUT          } from './modules/combine_outputs/main'

////////////////////////////////////////////////////
/* --           RUN MAIN WORKFLOW              -- */
////////////////////////////////////////////////////

workflow {

    // Bedfile channel to grab break bedfiles from data
    ch_bed_file = Channel.fromPath(params.bed_file + '/*.bed')

    // Grabbing the bedfile for performing bedtools intersect
    ch_intersect_bed_file = Channel.fromPath(params.bed_file_intersect)

    //Filter the reads for MapQ for >= 30 using python script 
    FILTER_READS(ch_bed_file)

    // Performing bedtools intersect with the supplied bedfile 
    INTERSECT_COUNTS(FILTER_READS.out.filt_bed_file, ch_intersect_bed_file)

    // Creating a temp channel with the initial data break bedfiles
    ch_bed_files_temp = ch_bed_file.map { tuple( it.simpleName, it ) }
        .groupTuple().set { bed_files }

    // Combining the above temp channel with the output from bedtools intersect process in order to pair the correctly numbered 
    // break bedfiles with filtered intersect files
    ch_bed_filt_comb = INTERSECT_COUNTS.out.filt_intersect_count_bed_files.map { tuple( it.simpleName - ~/_Sample$/, it ) }
        .combine( bed_files, by: 0 )
        .transpose( by: 2 )
        .map { sample, filt_bed, bed -> tuple( filt_bed, bed ) }

    // Normalise the counts generated by bedtools by using: counts/(total breaks/1000) in Python script and printing these into
    // individual text files
    COUNT_SUM_NORM(ch_bed_filt_comb)

    // Concatenating the outputs from count_sum_norm to create a single file with normalised counts that 
    // can be easily read into python for further analysis
    COMBINE_OUTPUT(COUNT_SUM_NORM.out.norm_counts.collect(sort:true))
}